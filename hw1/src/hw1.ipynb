{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(csv_path, audio_folder_path, sr=22050):\n",
    "    track_df = pd.read_csv(csv_path)\n",
    "\n",
    "    audio_dict = {\n",
    "        \"track\": [],\n",
    "        \"y\": [],\n",
    "        \"sr\": [],\n",
    "    }\n",
    "\n",
    "    for track_name in track_df[\"track\"]:\n",
    "        print(f\"loading {track_name}\")\n",
    "        y, sr = librosa.load(f\"{audio_folder_path}/{track_name}\", sr=sr)\n",
    "        audio_dict[\"track\"].append(track_name)\n",
    "        audio_dict[\"y\"].append(y)\n",
    "        audio_dict[\"sr\"].append(sr)\n",
    "\n",
    "    return track_df, pd.DataFrame(audio_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading normalize_5s_intro_thc1MtNagC8.wav\n",
      "loading normalize_5s_intro_Wo2qUD1g7xM.wav\n",
      "loading normalize_5s_intro_3ObVN3QQiZ8.wav\n",
      "loading normalize_5s_intro_S-zQJFRX5Fg.wav\n",
      "loading normalize_5s_intro_SyZOAgXiPMw.wav\n",
      "loading normalize_5s_intro_GQT8ejgV2_A.wav\n",
      "loading normalize_5s_intro_PQAIxeSIQU4.wav\n",
      "loading normalize_5s_intro_E-8pyVBvCPQ.wav\n",
      "loading normalize_5s_intro_Qr8eZSVaw10.wav\n",
      "loading normalize_5s_intro_p7j-tz1Cn4o.wav\n",
      "loading normalize_5s_intro_nISI4qF55F4.wav\n",
      "loading normalize_5s_intro_RoeRU5zxkak.wav\n",
      "loading normalize_5s_intro_EygNk739nnY.wav\n",
      "loading normalize_5s_intro_w1G3rqVil1s.wav\n",
      "loading normalize_5s_intro_KKc_RMln5UY.wav\n",
      "loading normalize_5s_intro_Ng2JdroNfC0.wav\n",
      "loading normalize_5s_intro_xc0sWhVhmkw.wav\n",
      "loading normalize_5s_intro_VVRszjvg3_U.wav\n",
      "loading normalize_5s_intro_C7u6rtswjCU.wav\n",
      "loading normalize_5s_intro_HiPkwl5p1GY.wav\n",
      "loading normalize_5s_intro_mYa_9d2Daas.wav\n",
      "loading normalize_5s_intro_6MSYrN4YfKY.wav\n",
      "loading normalize_5s_intro_O2q_9lBDM7I.wav\n",
      "loading normalize_5s_intro_7E_a_VKjcl8.wav\n",
      "loading normalize_5s_intro_a8cJLohQ_Jg.wav\n",
      "loading normalize_5s_intro_7zz-nEVKZdc.wav\n",
      "loading normalize_5s_intro_JeGhUESd_1o.wav\n",
      "loading normalize_5s_intro_IN1f9k8qVDk.wav\n",
      "loading normalize_5s_intro_RhBb77hG0iw.wav\n",
      "loading normalize_5s_intro_qAiwzv8N7rM.wav\n",
      "loading normalize_5s_intro_AoB8koE95C0.wav\n",
      "loading normalize_5s_intro_j3DigipQ_hQ.wav\n",
      "loading normalize_5s_intro_1X0SdKtnwo8.wav\n",
      "loading normalize_5s_intro_RCJx5VW-fQI.wav\n",
      "loading normalize_5s_intro_S_-qkv0NZ1g.wav\n",
      "loading normalize_5s_intro_C90sY_Ht6Ig.wav\n",
      "loading normalize_5s_intro_Z5gvqq3ChII.wav\n",
      "loading normalize_5s_intro_zumMQrI_tMg.wav\n",
      "loading normalize_5s_intro_gwsaElRJI2M.wav\n",
      "loading normalize_5s_intro_ftjEcrrf7r0.wav\n",
      "loading normalize_5s_intro_ZBBS4imv1qo.wav\n",
      "loading normalize_5s_intro_DyQ_9p6y89c.wav\n",
      "loading normalize_5s_intro_vgZv7Uu4YrA.wav\n",
      "loading normalize_5s_intro_wcLXjQLwSBE.wav\n",
      "loading normalize_5s_intro_7LuQQP-DAoc.wav\n",
      "loading normalize_5s_intro_BEo0rqOZIng.wav\n",
      "loading normalize_5s_intro_n4HTXYR-2AI.wav\n",
      "loading normalize_5s_intro_72T4j04MS8o.wav\n",
      "loading normalize_5s_intro_6TT_UgrRHq8.wav\n",
      "loading normalize_5s_intro_uo8qDCDZhK0.wav\n",
      "loading normalize_5s_intro_Et-YdmSo_3A.wav\n",
      "loading normalize_5s_intro_oxKbrl4kyg8.wav\n",
      "loading normalize_5s_intro_XgwqnGG-pbI.wav\n",
      "loading normalize_5s_intro_1wpJkzCWHcI.wav\n",
      "loading normalize_5s_intro_bwQ49N0jVvE.wav\n",
      "loading normalize_5s_intro_OMR2W-7AyYU.wav\n",
      "loading normalize_5s_intro_sjlkxcwhpwA.wav\n",
      "loading normalize_5s_intro_4F1wvsJXXVY.wav\n",
      "loading normalize_5s_intro_YEq-cvq_cK4.wav\n",
      "loading normalize_5s_intro_42O51bcJyq0.wav\n",
      "loading normalize_5s_intro_5FYAICvv-d0.wav\n",
      "loading normalize_5s_intro_yBzk2xXE9yg.wav\n",
      "loading normalize_5s_intro_zEWSSod0zTY.wav\n",
      "loading normalize_5s_intro_dvf--10EYXw.wav\n",
      "loading normalize_5s_intro_xQOXxmznGPg.wav\n",
      "loading normalize_5s_intro_hMWoOunsMFM.wav\n",
      "loading normalize_5s_intro_TnsOVDCq_b0.wav\n",
      "loading normalize_5s_intro_Yh78Ll6-ODQ.wav\n",
      "loading normalize_5s_intro_IYnu4-69fTA.wav\n",
      "loading normalize_5s_intro_SubIr_Fyp4M.wav\n",
      "loading normalize_5s_intro_WrRAZVJGImw.wav\n",
      "loading normalize_5s_intro_gFnNr5vr5bQ.wav\n",
      "loading normalize_5s_intro_j9KKh215HTs.wav\n",
      "loading normalize_5s_intro_XBTT9tSVsh0.wav\n",
      "loading normalize_5s_intro_u8BxVzRG9bE.wav\n",
      "loading normalize_5s_intro_SQBuVfTX1ME.wav\n",
      "loading normalize_5s_intro_-MqZKMbOYEA.wav\n",
      "loading normalize_5s_intro_IpniN1Wq68Y.wav\n",
      "loading normalize_5s_intro_1lunUbvf35M.wav\n",
      "loading normalize_5s_intro_zk04E79riMQ.wav\n",
      "loading normalize_5s_intro_uUfPwlxFFJM.wav\n",
      "loading normalize_5s_intro_Ws-QlpSltr8.wav\n",
      "loading normalize_5s_intro_xT1eOeXlTXg.wav\n",
      "loading normalize_5s_intro_1Ngn3fZIK2E.wav\n",
      "loading normalize_5s_intro_2JL_KcEzkqg.wav\n",
      "loading normalize_5s_intro_4jvQFLlRQlo.wav\n",
      "loading normalize_5s_intro_AjGkbFqi67c.wav\n",
      "loading normalize_5s_intro_ahpmuikko3U.wav\n",
      "loading normalize_5s_intro_sY5wXfgspQI.wav\n",
      "loading normalize_5s_intro_HMvXE4Zs6ZA.wav\n",
      "loading normalize_5s_intro_gv7BRXvZJbI.wav\n",
      "loading normalize_5s_intro_4wgo8K28RNM.wav\n",
      "loading normalize_5s_intro_2ySLmwsfP4Q.wav\n",
      "loading normalize_5s_intro_MY4YJxn-9Og.wav\n",
      "loading normalize_5s_intro_3gjfHYZ873o.wav\n",
      "loading normalize_5s_intro_csHiDQXIggE.wav\n",
      "loading normalize_5s_intro_C5VCGM2J5ls.wav\n",
      "loading normalize_5s_intro_ey4Fc9DP5Rw.wav\n",
      "loading normalize_5s_intro_bI7xde9-3BI.wav\n",
      "loading normalize_5s_intro_EfZ-dVDySzc.wav\n",
      "loading normalize_5s_intro_Zh3uBgwow8A.wav\n",
      "loading normalize_5s_intro_JQTlG7NxJek.wav\n",
      "loading normalize_5s_intro_1CrxzClzLvs.wav\n",
      "loading normalize_5s_intro_0aC-jOKuBFE.wav\n",
      "loading normalize_5s_intro_xePw8n4xu8o.wav\n",
      "loading normalize_5s_intro_lEHM9HZf0IA.wav\n",
      "loading normalize_5s_intro_xhmtXrtLkgo.wav\n",
      "loading normalize_5s_intro_hHItMz0gfaU.wav\n",
      "loading normalize_5s_intro_99f0oH45TVc.wav\n",
      "loading normalize_5s_intro_co6WMzDOh1o.wav\n",
      "loading normalize_5s_intro_xqzOxMdhmzU.wav\n",
      "loading normalize_5s_intro_h-nnAeByB1A.wav\n",
      "loading normalize_5s_intro_TFv9Kcym9dg.wav\n",
      "loading normalize_5s_intro_tEW2eRQ-4DY.wav\n",
      "loading normalize_5s_intro_VAc0xuVa7jI.wav\n",
      "loading normalize_5s_intro_PALMMqZLAQk.wav\n",
      "loading normalize_5s_intro_STpRa2JPFA0.wav\n",
      "loading normalize_5s_intro_SgJMnEdtTXA.wav\n",
      "loading normalize_5s_intro_NL2ZHPji3Z0.wav\n",
      "loading normalize_5s_intro_EVSuxb6Ywcg.wav\n",
      "loading normalize_5s_intro_wAJMhJpSCIc.wav\n",
      "loading normalize_5s_intro_GphIn74Weu0.wav\n",
      "loading normalize_5s_intro_gue_crpFdSE.wav\n",
      "loading normalize_5s_intro_oQ0O2cd1T04.wav\n",
      "loading normalize_5s_intro_vMcFA2x23FE.wav\n",
      "loading normalize_5s_intro_FhvXg70ycrM.wav\n",
      "loading normalize_5s_intro_lE_747E_Sdg.wav\n",
      "loading normalize_5s_intro_i0MrGb1hT2U.wav\n",
      "loading normalize_5s_intro_bI8-2blisUM.wav\n",
      "loading normalize_5s_intro_aQ06TfyA1Ks.wav\n",
      "loading normalize_5s_intro_ZvrysfBDzSs.wav\n",
      "loading normalize_5s_intro_v2seHL0pwbg.wav\n",
      "loading normalize_5s_intro_BrrWNfjgHGs.wav\n",
      "loading normalize_5s_intro_j1c70vRHdhQ.wav\n",
      "loading normalize_5s_intro_3DCHLwOqtJs.wav\n",
      "loading normalize_5s_intro_g20t_K9dlhU.wav\n",
      "loading normalize_5s_intro_EH1OEWJ9C5w.wav\n",
      "loading normalize_5s_intro_SCBxmcwmX7U.wav\n",
      "loading normalize_5s_intro_tXvpe2GbUec.wav\n",
      "loading normalize_5s_intro_7ZgPGMfUVek.wav\n",
      "loading normalize_5s_intro_aIJuCcGFJkc.wav\n",
      "loading normalize_5s_intro_RLMl1umHgp0.wav\n",
      "loading normalize_5s_intro_KT-m6qTJyN0.wav\n",
      "loading normalize_5s_intro_WJs-_T8I74Y.wav\n",
      "loading normalize_5s_intro_aIyqRdrHodE.wav\n",
      "loading normalize_5s_intro_XJT-fM4nBJU.wav\n",
      "loading normalize_5s_intro_7QQzDQceGgU.wav\n",
      "loading normalize_5s_intro_fE2h3lGlOsk.wav\n",
      "loading normalize_5s_intro_Oq1n8fUxQZc.wav\n",
      "loading normalize_5s_intro_pssWSj42t8M.wav\n",
      "loading normalize_5s_intro_GsPq9mzFNGY.wav\n",
      "loading normalize_5s_intro_Jg9NbDizoPM.wav\n",
      "loading normalize_5s_intro_Ib7m3Qh-4O4.wav\n",
      "loading normalize_5s_intro_hn3wJ1_1Zsg.wav\n",
      "loading normalize_5s_intro_hjZqVw3qI9E.wav\n",
      "loading normalize_5s_intro_cUKD9tEeBp0.wav\n",
      "loading normalize_5s_intro_q_4no3KCrY4.wav\n",
      "loading normalize_5s_intro_VlWs8ey2nyg.wav\n",
      "loading normalize_5s_intro_Srp0opA8V8o.wav\n",
      "loading normalize_5s_intro_PYM9NUU9Roc.wav\n",
      "loading normalize_5s_intro_v0UvOsCi8mc.wav\n",
      "loading normalize_5s_intro_zaCbuB3w0kg.wav\n",
      "loading normalize_5s_intro_PCp2iXA1uLE.wav\n",
      "loading normalize_5s_intro_S2RnxiNJg0M.wav\n",
      "loading normalize_5s_intro_Jtv4satRsP0.wav\n",
      "loading normalize_5s_intro_ytq5pGcM77w.wav\n",
      "loading normalize_5s_intro_9nWpMZFrbvI.wav\n",
      "loading normalize_5s_intro_1kN-34GFMYM.wav\n",
      "loading normalize_5s_intro_Yyvo9O8fN-A.wav\n",
      "loading normalize_5s_intro_ulj-L3K_Gzs.wav\n",
      "loading normalize_5s_intro_V-ar6MLjy5o.wav\n",
      "loading normalize_5s_intro_dtOv6WvJ44w.wav\n",
      "loading normalize_5s_intro_XkC8Uzl9pCY.wav\n",
      "loading normalize_5s_intro_jII5qoCrzYE.wav\n",
      "loading normalize_5s_intro_7pcZIsJNlAs.wav\n",
      "loading normalize_5s_intro_0QN9KLFWn7I.wav\n",
      "loading normalize_5s_intro_d6BzCEkGd3I.wav\n",
      "loading normalize_5s_intro_lYxcW8jtFw0.wav\n",
      "loading normalize_5s_intro_R1T_SrdQGH8.wav\n",
      "loading normalize_5s_intro_YOKq1VmEbtc.wav\n",
      "loading normalize_5s_intro_19Q9l85Feqw.wav\n",
      "loading normalize_5s_intro_CXm7hPs_als.wav\n",
      "loading normalize_5s_intro_nFOLhtsyvMA.wav\n",
      "loading normalize_5s_intro_-8cFfkyk7vA.wav\n",
      "loading normalize_5s_intro_ZIiQ1jMqhVM.wav\n",
      "loading normalize_5s_intro_hejXc_FSYb8.wav\n",
      "loading normalize_5s_intro_eXvBjCO19QY.wav\n",
      "loading normalize_5s_intro_haCay85cpvo.wav\n",
      "loading normalize_5s_intro_RpJz01guPMY.wav\n",
      "loading normalize_5s_intro_sPlXrbVLdO8.wav\n",
      "loading normalize_5s_intro_Mme9REVuidw.wav\n",
      "loading normalize_5s_intro_UGTYqTKUl8w.wav\n",
      "loading normalize_5s_intro_9DP0yMwvyWE.wav\n",
      "loading normalize_5s_intro_WrDJMxSKlCA.wav\n",
      "loading normalize_5s_intro_2F8Kr91wQ0U.wav\n",
      "loading normalize_5s_intro_gyegm85BPPA.wav\n",
      "loading normalize_5s_intro_Xhh3_-JRnDc.wav\n",
      "loading normalize_5s_intro_WRSeV_27z6k.wav\n",
      "loading normalize_5s_intro_HwcCBnfhsR4.wav\n",
      "loading normalize_5s_intro_bd5m12UEHWI.wav\n",
      "loading normalize_5s_intro_1juIFmPyG-Y.wav\n",
      "loading normalize_5s_intro_DGsoqhIUgDQ.wav\n",
      "loading normalize_5s_intro_2UL-1MOlSPw.wav\n",
      "loading normalize_5s_intro_2AWE9tqnDPw.wav\n",
      "loading normalize_5s_intro_68b_HImZAig.wav\n",
      "loading normalize_5s_intro_GIulOhzXufc.wav\n",
      "loading normalize_5s_intro_Stet_4bnclk.wav\n",
      "loading normalize_5s_intro_RHGfkuJv0j0.wav\n",
      "loading normalize_5s_intro_0uLI6BnVh6w.wav\n",
      "loading normalize_5s_intro_uo6VU4euIbY.wav\n",
      "loading normalize_5s_intro_6pARjpdqxYQ.wav\n",
      "loading normalize_5s_intro_hjIhCG_nIPA.wav\n",
      "loading normalize_5s_intro_hV-FwW1LgxU.wav\n",
      "loading normalize_5s_intro_mWfWyhzC22U.wav\n",
      "loading normalize_5s_intro_IISA6t-9zzc.wav\n",
      "loading normalize_5s_intro_gDevCxVY_wA.wav\n",
      "loading normalize_5s_intro_IrtcCSE2bVY.wav\n",
      "loading normalize_5s_intro_feVUoKhP1mE.wav\n",
      "loading normalize_5s_intro_Tfypj4UwvvA.wav\n",
      "loading normalize_5s_intro_TeH7sCVCMJk.wav\n"
     ]
    }
   ],
   "source": [
    "data_folder_path = \"../data\"\n",
    "sr = 22050\n",
    "track_df, audio_df = read_files(f\"{data_folder_path}/train.csv\", f\"{data_folder_path}/audios/clips\", sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(track_df, audio_df):\n",
    "    # x = np.array([[librosa.feature.melspectrogram(y=y, sr=sr)] for y, sr in zip(audio_df[\"y\"], audio_df[\"sr\"])])\n",
    "    x = np.array([[[value]] for value in audio_df[\"y\"].values])\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape = (220, 1, 1, 110250)\n",
      "y shape = (220, 1)\n"
     ]
    }
   ],
   "source": [
    "# print(audio_df)\n",
    "# for index, (track, y, sr) in audio_df.iterrows():\n",
    "#     print(y)\n",
    "\n",
    "x = preprocess_data(track_df, audio_df)\n",
    "y = np.array([track_df['score'].values]).T\n",
    "\n",
    "# print(x)\n",
    "print(\"x shape =\", x.shape)\n",
    "print(\"y shape =\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_nothing(y, sr):\n",
    "    return y\n",
    "\n",
    "\n",
    "def augmentation_change_amplitude(y, sr):\n",
    "    gain = random.uniform(0.8, 1.25)\n",
    "    return y * gain\n",
    "\n",
    "\n",
    "def augmentation_inverse(y, sr):\n",
    "    return y * -1\n",
    "\n",
    "\n",
    "def augmentation_noise(y, sr):\n",
    "    signal = y[0][0]\n",
    "    RMS = np.sqrt(np.mean(signal**2))\n",
    "    STD_n = random.uniform(0, 0.01)\n",
    "    noise = np.random.normal(0, STD_n, signal.shape[0])\n",
    "    signal_noise = signal+noise\n",
    "    return np.array([[signal_noise]])\n",
    "\n",
    "\n",
    "def augmentation_pitch(y, sr):\n",
    "    signal = y[0][0]\n",
    "    pitch_factor = random.uniform(-10, 10)\n",
    "    return np.array([[librosa.effects.pitch_shift(signal, sr=sr, n_steps=pitch_factor)]])\n",
    "\n",
    "\n",
    "def augmentation_speed(y, sr):\n",
    "    signal = y[0][0]\n",
    "    speed_factor = random.uniform(0.5, 2)\n",
    "    return np.array([[librosa.effects.time_stretch(signal, rate=speed_factor)]])\n",
    "\n",
    "\n",
    "def augmentation_func(y, sr):\n",
    "    if random.choice([True, False]):\n",
    "        y = augmentation_change_amplitude(y, sr)\n",
    "    if random.choice([True, False]):\n",
    "        y = augmentation_inverse(y, sr)\n",
    "    if random.choice([True, False]):\n",
    "        y = augmentation_noise(y, sr)\n",
    "    if random.choice([True, False]):\n",
    "        y = augmentation_pitch(y, sr)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y, augmentation = False):\n",
    "        self.x = x\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.len = x.shape[0]\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        if(self.augmentation):\n",
    "            x = augmentation_func(x, sr)\n",
    "        \n",
    "        return torch.tensor(x, dtype=torch.float32), self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation testing\n",
      "train_set\n",
      "tensor([ 0.0146,  0.0104,  0.0135,  ..., -0.0471, -0.0485, -0.0632])\n",
      "tensor([-0.0137, -0.0127, -0.0031,  ...,  0.0475,  0.0520,  0.0631])\n",
      "tensor([-0.0077, -0.0117, -0.0104,  ...,  0.0459,  0.0498,  0.0660])\n",
      "tensor([ 0.0080,  0.0120,  0.0077,  ..., -0.0495, -0.0451, -0.0620])\n",
      "tensor([ 0.0081,  0.0131,  0.0103,  ..., -0.0584, -0.0596, -0.0791])\n",
      "tensor([-0.0094, -0.0198, -0.0059,  ...,  0.0456,  0.0352,  0.0610])\n",
      "tensor([ 0.0071,  0.0111,  0.0102,  ..., -0.0208, -0.0146,  0.0000])\n",
      "tensor([-0.0077, -0.0117, -0.0104,  ...,  0.0459,  0.0498,  0.0660])\n",
      "tensor([ 0.0077,  0.0117,  0.0104,  ..., -0.0459, -0.0498, -0.0660])\n",
      "tensor([ 0.0138,  0.0129,  0.0130,  ..., -0.0550, -0.0498, -0.0804])\n",
      "test_set\n",
      "tensor([-4.8946e-05, -1.3814e-05,  1.8567e-06,  ..., -1.2329e-02,\n",
      "        -1.2139e-02, -3.1391e-02])\n",
      "tensor([-4.8946e-05, -1.3814e-05,  1.8567e-06,  ..., -1.2329e-02,\n",
      "        -1.2139e-02, -3.1391e-02])\n",
      "tensor([-4.8946e-05, -1.3814e-05,  1.8567e-06,  ..., -1.2329e-02,\n",
      "        -1.2139e-02, -3.1391e-02])\n",
      "tensor([-4.8946e-05, -1.3814e-05,  1.8567e-06,  ..., -1.2329e-02,\n",
      "        -1.2139e-02, -3.1391e-02])\n",
      "tensor([-4.8946e-05, -1.3814e-05,  1.8567e-06,  ..., -1.2329e-02,\n",
      "        -1.2139e-02, -3.1391e-02])\n",
      "tensor([-4.8946e-05, -1.3814e-05,  1.8567e-06,  ..., -1.2329e-02,\n",
      "        -1.2139e-02, -3.1391e-02])\n",
      "tensor([-4.8946e-05, -1.3814e-05,  1.8567e-06,  ..., -1.2329e-02,\n",
      "        -1.2139e-02, -3.1391e-02])\n",
      "tensor([-4.8946e-05, -1.3814e-05,  1.8567e-06,  ..., -1.2329e-02,\n",
      "        -1.2139e-02, -3.1391e-02])\n",
      "tensor([-4.8946e-05, -1.3814e-05,  1.8567e-06,  ..., -1.2329e-02,\n",
      "        -1.2139e-02, -3.1391e-02])\n",
      "tensor([-4.8946e-05, -1.3814e-05,  1.8567e-06,  ..., -1.2329e-02,\n",
      "        -1.2139e-02, -3.1391e-02])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_test_ratio = 0.85\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=train_test_ratio, random_state=0)\n",
    "\n",
    "train_set = MyDataset(x=x_train, y=y_train, augmentation=True)\n",
    "test_set = MyDataset(x=x_test, y=y_test, augmentation=False)\n",
    "\n",
    "# train_size = len(x_train)\n",
    "# test_size =  len(x_test)\n",
    "\n",
    "# train_set, _ = torch.utils.data.random_split(train_set, [train_size, 0])\n",
    "# test_set, _ = torch.utils.data.random_split(test_set, [test_size, 0])\n",
    "\n",
    "\n",
    "print(\"Augmentation testing\")\n",
    "print(\"train_set\")\n",
    "for i in range(10):\n",
    "    print(train_set[0][0][0][0])\n",
    "\n",
    "print(\"test_set\")\n",
    "for i in range(10):\n",
    "    print(test_set[0][0][0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Model, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 16, (5, 5), 1)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, (5, 5), 1)\n",
    "#         self.pool = nn.MaxPool2d((2, 2), 2)\n",
    "#         self.fc1 = nn.Linear(199680, 1024)\n",
    "#         self.dropout1 = nn.Dropout(0.25)\n",
    "#         self.fc2 = nn.Linear(1024, 512)\n",
    "#         self.dropout2 = nn.Dropout(0.25)\n",
    "#         self.fc3 = nn.Linear(512, 1)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.pool(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.fc1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.dropout1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.fc3(x)\n",
    "#         output = self.sigmoid(x)\n",
    "#         return output\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, 32), 1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, (1, 32), 1)\n",
    "        self.pool1 = nn.MaxPool2d((1, 2), 2)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(1763008, 256)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        # x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        output = self.sigmoid(x)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, epoch, log_interval=10):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(f\"output = {output} target = {target} loss = {loss}\")\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device,criterion, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()  # sum up batch loss\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(1, 32), stride=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(1, 32), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=(1, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (fc1): Linear(in_features=1763008, out_features=256, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "gamma = 0.1\n",
    "epochs = 50\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model().to(device)\n",
    "print(model)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/187 (0%)]\tLoss: 0.143834\n",
      "Train Epoch: 1 [100/187 (53%)]\tLoss: 0.000399\n",
      "\n",
      "Test set: Average loss: 0.0256\n",
      "\n",
      "Train Epoch: 2 [0/187 (0%)]\tLoss: 0.008330\n",
      "Train Epoch: 2 [100/187 (53%)]\tLoss: 0.163736\n",
      "\n",
      "Test set: Average loss: 0.0265\n",
      "\n",
      "Train Epoch: 3 [0/187 (0%)]\tLoss: 0.028389\n",
      "Train Epoch: 3 [100/187 (53%)]\tLoss: 0.037846\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 4 [0/187 (0%)]\tLoss: 0.002842\n",
      "Train Epoch: 4 [100/187 (53%)]\tLoss: 0.021132\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 5 [0/187 (0%)]\tLoss: 0.039216\n",
      "Train Epoch: 5 [100/187 (53%)]\tLoss: 0.003739\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 6 [0/187 (0%)]\tLoss: 0.000031\n",
      "Train Epoch: 6 [100/187 (53%)]\tLoss: 0.028373\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 7 [0/187 (0%)]\tLoss: 0.000250\n",
      "Train Epoch: 7 [100/187 (53%)]\tLoss: 0.009639\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 8 [0/187 (0%)]\tLoss: 0.030238\n",
      "Train Epoch: 8 [100/187 (53%)]\tLoss: 0.023427\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 9 [0/187 (0%)]\tLoss: 0.000213\n",
      "Train Epoch: 9 [100/187 (53%)]\tLoss: 0.070780\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 10 [0/187 (0%)]\tLoss: 0.150008\n",
      "Train Epoch: 10 [100/187 (53%)]\tLoss: 0.057516\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 11 [0/187 (0%)]\tLoss: 0.090928\n",
      "Train Epoch: 11 [100/187 (53%)]\tLoss: 0.026858\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 12 [0/187 (0%)]\tLoss: 0.000119\n",
      "Train Epoch: 12 [100/187 (53%)]\tLoss: 0.014312\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 13 [0/187 (0%)]\tLoss: 0.003729\n",
      "Train Epoch: 13 [100/187 (53%)]\tLoss: 0.002140\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 14 [0/187 (0%)]\tLoss: 0.049780\n",
      "Train Epoch: 14 [100/187 (53%)]\tLoss: 0.013758\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 15 [0/187 (0%)]\tLoss: 0.001370\n",
      "Train Epoch: 15 [100/187 (53%)]\tLoss: 0.014088\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 16 [0/187 (0%)]\tLoss: 0.005850\n",
      "Train Epoch: 16 [100/187 (53%)]\tLoss: 0.001331\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 17 [0/187 (0%)]\tLoss: 0.001100\n",
      "Train Epoch: 17 [100/187 (53%)]\tLoss: 0.011022\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 18 [0/187 (0%)]\tLoss: 0.005460\n",
      "Train Epoch: 18 [100/187 (53%)]\tLoss: 0.000865\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 19 [0/187 (0%)]\tLoss: 0.027439\n",
      "Train Epoch: 19 [100/187 (53%)]\tLoss: 0.000058\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 20 [0/187 (0%)]\tLoss: 0.052043\n",
      "Train Epoch: 20 [100/187 (53%)]\tLoss: 0.038045\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 21 [0/187 (0%)]\tLoss: 0.003130\n",
      "Train Epoch: 21 [100/187 (53%)]\tLoss: 0.005637\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 22 [0/187 (0%)]\tLoss: 0.007957\n",
      "Train Epoch: 22 [100/187 (53%)]\tLoss: 0.001171\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 23 [0/187 (0%)]\tLoss: 0.101159\n",
      "Train Epoch: 23 [100/187 (53%)]\tLoss: 0.000092\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 24 [0/187 (0%)]\tLoss: 0.003399\n",
      "Train Epoch: 24 [100/187 (53%)]\tLoss: 0.044789\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 25 [0/187 (0%)]\tLoss: 0.016679\n",
      "Train Epoch: 25 [100/187 (53%)]\tLoss: 0.000338\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 26 [0/187 (0%)]\tLoss: 0.014124\n",
      "Train Epoch: 26 [100/187 (53%)]\tLoss: 0.070922\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 27 [0/187 (0%)]\tLoss: 0.027369\n",
      "Train Epoch: 27 [100/187 (53%)]\tLoss: 0.044229\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 28 [0/187 (0%)]\tLoss: 0.077784\n",
      "Train Epoch: 28 [100/187 (53%)]\tLoss: 0.009153\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 29 [0/187 (0%)]\tLoss: 0.034214\n",
      "Train Epoch: 29 [100/187 (53%)]\tLoss: 0.002222\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 30 [0/187 (0%)]\tLoss: 0.004625\n",
      "Train Epoch: 30 [100/187 (53%)]\tLoss: 0.004102\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 31 [0/187 (0%)]\tLoss: 0.102280\n",
      "Train Epoch: 31 [100/187 (53%)]\tLoss: 0.004976\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 32 [0/187 (0%)]\tLoss: 0.013397\n",
      "Train Epoch: 32 [100/187 (53%)]\tLoss: 0.000678\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 33 [0/187 (0%)]\tLoss: 0.014445\n",
      "Train Epoch: 33 [100/187 (53%)]\tLoss: 0.201802\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 34 [0/187 (0%)]\tLoss: 0.000806\n",
      "Train Epoch: 34 [100/187 (53%)]\tLoss: 0.117713\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 35 [0/187 (0%)]\tLoss: 0.001552\n",
      "Train Epoch: 35 [100/187 (53%)]\tLoss: 0.001477\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 36 [0/187 (0%)]\tLoss: 0.006778\n",
      "Train Epoch: 36 [100/187 (53%)]\tLoss: 0.012877\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 37 [0/187 (0%)]\tLoss: 0.000027\n",
      "Train Epoch: 37 [100/187 (53%)]\tLoss: 0.000001\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 38 [0/187 (0%)]\tLoss: 0.018573\n",
      "Train Epoch: 38 [100/187 (53%)]\tLoss: 0.033211\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 39 [0/187 (0%)]\tLoss: 0.000188\n",
      "Train Epoch: 39 [100/187 (53%)]\tLoss: 0.003603\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 40 [0/187 (0%)]\tLoss: 0.063918\n",
      "Train Epoch: 40 [100/187 (53%)]\tLoss: 0.000285\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 41 [0/187 (0%)]\tLoss: 0.001468\n",
      "Train Epoch: 41 [100/187 (53%)]\tLoss: 0.100664\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 42 [0/187 (0%)]\tLoss: 0.038000\n",
      "Train Epoch: 42 [100/187 (53%)]\tLoss: 0.000377\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 43 [0/187 (0%)]\tLoss: 0.003644\n",
      "Train Epoch: 43 [100/187 (53%)]\tLoss: 0.040960\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 44 [0/187 (0%)]\tLoss: 0.021927\n",
      "Train Epoch: 44 [100/187 (53%)]\tLoss: 0.026940\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 45 [0/187 (0%)]\tLoss: 0.006881\n",
      "Train Epoch: 45 [100/187 (53%)]\tLoss: 0.004929\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 46 [0/187 (0%)]\tLoss: 0.009007\n",
      "Train Epoch: 46 [100/187 (53%)]\tLoss: 0.055154\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 47 [0/187 (0%)]\tLoss: 0.017654\n",
      "Train Epoch: 47 [100/187 (53%)]\tLoss: 0.000558\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 48 [0/187 (0%)]\tLoss: 0.029859\n",
      "Train Epoch: 48 [100/187 (53%)]\tLoss: 0.033626\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 49 [0/187 (0%)]\tLoss: 0.000828\n",
      "Train Epoch: 49 [100/187 (53%)]\tLoss: 0.008600\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n",
      "Train Epoch: 50 [0/187 (0%)]\tLoss: 0.014692\n",
      "Train Epoch: 50 [100/187 (53%)]\tLoss: 0.002828\n",
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, criterion, optimizer, epoch, log_interval=100)\n",
    "    test(model, device, criterion, test_loader)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, device, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder_path = \"../model\"\n",
    "save_model_name = \"model1.pt\"\n",
    "\n",
    "torch.save(model, f\"{model_folder_path}/{save_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading normalize_5s_intro_0EVVKs6DQLo.wav\n",
      "loading normalize_5s_intro_d7to9URtLZ4.wav\n",
      "loading normalize_5s_intro_TzhhbYS9EO4.wav\n",
      "loading normalize_5s_intro_nn5nypm7GG8.wav\n",
      "loading normalize_5s_intro_hed6HkYNA7g.wav\n",
      "loading normalize_5s_intro_rWznOAwxM1g.wav\n",
      "loading normalize_5s_intro_zyQkFh-E4Ak.wav\n",
      "loading normalize_5s_intro_agKkcRXN2iE.wav\n",
      "loading normalize_5s_intro_SZaZU_qi6Xc.wav\n",
      "loading normalize_5s_intro_ZpDQJnI4OhU.wav\n",
      "loading normalize_5s_intro_D4nWzd63jV4.wav\n",
      "loading normalize_5s_intro_9odM1BRqop4.wav\n",
      "loading normalize_5s_intro_F64yFFnZfkI.wav\n",
      "loading normalize_5s_intro_Js2JQH_kt0I.wav\n",
      "loading normalize_5s_intro_Skt_NKI4d6U.wav\n"
     ]
    }
   ],
   "source": [
    "data_folder_path = \"../data\"\n",
    "model_folder_path = \"../model\"\n",
    "load_model_name = \"model1.pt\"\n",
    "# sr = 11025\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model = torch.load(f\"{model_folder_path}/{load_model_name}\")\n",
    "model.eval()\n",
    "\n",
    "test_track_df, test_audio_df = read_files(f\"{data_folder_path}/test.csv\", f\"{data_folder_path}/audios/clips\", sr=sr)\n",
    "test_x = preprocess_data(test_track_df, test_audio_df)\n",
    "\n",
    "output_dict = {\n",
    "    \"track\": [],\n",
    "    \"score\": []\n",
    "}\n",
    "\n",
    "for track, features in zip(test_track_df['track'], test_x):\n",
    "    features = np.array([features])\n",
    "    features = torch.tensor(features, dtype=torch.float32).to(device)\n",
    "    score = model(features)\n",
    "    output_dict[\"track\"].append(track)\n",
    "    output_dict[\"score\"].append(score[0][0].cpu().detach().numpy())\n",
    "\n",
    "output_df = pd.DataFrame(output_dict)\n",
    "output_df.to_csv(f\"{data_folder_path}/submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Change | loss   |\n",
    "| ------ | ------ |\n",
    "| baseline | 0.0269 |\n",
    "| Move dropout | 0.0272 |\n",
    "| 3 conv 2 fc | 0.0269 |\n",
    "| 3 conv 3 fc | 0.0260 |\n",
    "| 2 conv 3 fc | 0.0258 |\n",
    "| 2 conv 2 fc | 0.0530 |\n",
    "| augmentation | 0.0268 |\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86eeccd87f0af4babcb8dbd19537ae118249bd36969b4c7d14d82db374bb1475"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('hw1-82sFHoS4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
